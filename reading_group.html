---
title: Reading Group
---
{% capture template_content %}
  <section class="reading-group">
    {% include frags/section_header.html title="Reading Group" %}
    <p>
      This page is dedicated to the NLP reading group @ Universit√© de Lorraine.
      Meetings are scheduled every other Thursday.
    </p>
    <p>
      This reading group is a spiritual child of last year's reading group,
      organized by Hoa Le You can find all the relevant information on that
      previous reading group <a href="https://github.com/lethienhoa/Deep-Learning-and-Natural-Language-Understanding-Reading-Group" >
      here</a>.
    </p>
    <p>
      One of the goals of this reading group is to provide a space dedicated to
      discussion for the different NLP labs across Nancy (Synalp, Atilf, ...).
      To that end, we try to make the presentations as inclusive as possible.
    </p>
    <p>
      If you'd like to present something, but can't think of a paper to present,
      <a href="https://docs.google.com/spreadsheets/d/1Mu5eCprPaQ8wv_XmF3-8jPFychaYZFZ8ftU2pYSReUc/edit#gid=0">there</a>'s
      a spreadsheet with some suggestions. The sheet is editable: don't hesitate to add
      a suggestion yourself!
    </p>
    <div class="commands-container">
      <table class="reading-group-session-commands">
        <tbody>
          <tr>
            <td>
              <a class="reversor">Reverse sessions</a>
            </td>
            <td>
              <a class="collapsor">Collapse all</a>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
    <div class="container">
      {% capture sessiondate %}
        {{"February 14, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% capture supcontent %}
        <p>
            A recent advance in the field of word embeddings is the rise of
            contextualized embeddings. This first session of the reading group
            was dedicated to one of the most popular contextual embedding
            architecture, BERT.
        </p>
        <p>
            BERT is also related to the
            <a href="https://arxiv.org/abs/1706.03762" >Transformer architecture
            of Vaswani and colleagues</a>. See also the
            <a href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL of
            Dai and al</a>.
            <a href="https://github.com/huggingface/pytorch-pretrained-BERT">Here</a>'s a github of variations on the Transformer.
        </p>
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          speaker="Timothee Mickus"
          paper="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
          paper_link="https://arxiv.org/pdf/1810.04805.pdf"
          slides_link="reading_group/slides/tmickus_140219.pdf"
          slides_downloadable=true
          supplementary_content=supcontent
      %}

      {% capture sessiondate %}
        {{"February 28, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% capture supcontent %}
        <p>
            Sentence embeddings have been a focus of research over the past few
            years, with new algorithms, datasets and benchmarks. The paper for
            this second session takes a step back and tries to evaluate how much
            these sentence embeddings actually gain over random methods.
        </p>
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          speaker="Timothee Mickus"
          paper="No Training Required: Exploring Random Encoders for Sentence Classification"
          paper_link="https://arxiv.org/pdf/1901.10444.pdf"
          slides_link="reading_group/slides/tmickus_280219.pdf"
          slides_downloadable=true
          supplementary_content=supcontent
      %}
      {% capture sessiondate %}
        {{"March 14, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          speaker="Angela Fan"
          paper="Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"
          paper_link="https://nlp.stanford.edu/pubs/khandelwal2018lm.pdf"
          slides_link="reading_group/slides/afan_140319.pdf"
          slides_downloadable=true
      %}
      {% capture sessiondate %}
        {{"March 28, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          speaker="Hoa Le"
          paper="Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling"
          paper_link="https://www.aclweb.org/anthology/D17-1159"
          slides_link="https://docs.google.com/presentation/d/1R7wlD8yYmvEuJc0ALfat04vK4ER9mS7aFAak8gzruyY/edit#slide=id.p"
          slides_downloadable=false
      %}
      {% capture sessiondate %}
        {{"April 11, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          location="Loria, B009"
          speaker="Anastasia Shimorina"
          paper="Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages"
          paper_link="https://arxiv.org/pdf/1903.06400.pdf"
          slides_link="reading_group/slides/ashimorina_110419.pdf"
          slides_downloadable=true
          hour="14:30"
      %}
      {% capture sessiondate %}
        {{"April 25, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          location="Loria, A015"
          speaker="Hoa Le"
          slides_link="https://drive.google.com/open?id=1NVhfUt3teT8Bl5Ni5VuKLdUNTBYuj5VXU5K0x0jCDLk"
          slides_downloadable=false
          hour="14:30"
      %}
      {% capture sessiondate %}
        {{"June 13, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          location="Loria, A217"
          speaker="Timothee Mickus"
          paper="Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
          paper_link="https://arxiv.org/pdf/1905.09418.pdf"
          slides_link="reading_group/slides/tmickus_130619.pdf"
          slides_downloadable=true
          hour="10:30"
      %}
      {% capture sessiondate %}
        {{"September 13, 2019" | date: "%b %d, %y" }}
      {% endcapture %}
      {% include reading_group/reading_group_session.html
          date=sessiondate
          location="Loria, B009"
          speaker="Anastasia Shimorina"
          paper="Some papers from ACL"
          hour="14:30"
      %}
    </div>
  </section>
{% endcapture %}
{% include frags/template.html template_content=template_content %}
